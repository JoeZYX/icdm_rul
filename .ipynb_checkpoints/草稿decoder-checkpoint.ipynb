{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试liear\n",
    "input_x = torch.tensor([[[1,1],[2,2],[3,3]],[[1,1],[2,2],[3,3]]]).double()\n",
    "pro = nn.Linear(2, 1, bias=True).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6162, -0.5947, -0.5731],\n",
       "        [-0.6162, -0.5947, -0.5731]], dtype=torch.float64,\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro(input_x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "from models.Attention import AttentionLayer,MaskAttention,ProbAttention\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 self_attention_layer_types, \n",
    "                 cross_attention_layer_types, \n",
    "                 d_model,\n",
    "                 n_heads,                # 和上面一样，所以需要是 类型个数的倍数\n",
    "                 d_ff=None,              # 因为attention不改变维度，在最后forward的时候，中间过程的维度\n",
    "                 dropout=0.1,  \n",
    "                 activation=\"relu\", \n",
    "                 forward_kernel_size = 1, \n",
    "                 value_kernel_size=1,\n",
    "                 causal_kernel_size = 3,\n",
    "                 output_attention=True):\n",
    "                 #norm =\"layer\", \n",
    "                 #se_block =False):\n",
    "\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        nr_heads_type = len(cross_attention_layer_types)\n",
    "        heads_each_type = int(n_heads/nr_heads_type)\n",
    "        d_model_each_type = int(d_model/nr_heads_type)\n",
    "        \n",
    "        #self.norm = norm\n",
    "        #self.se_block = se_block\n",
    "        self.self_attention_layer_types = self_attention_layer_types\n",
    "        self.cross_attention_layer_types = cross_attention_layer_types\n",
    "        \n",
    "        \n",
    "        # 第一部分，进行attention\n",
    "        # ----------- self_attention_layer_types ---------------\n",
    "        if len(self.self_attention_layer_types)>0:\n",
    "            self_attention_layer_list = []\n",
    "            for type_attn in self_attention_layer_types:\n",
    "                if type_attn==\"ProbMask\":\n",
    "                    self_attention_layer_list.append(AttentionLayer(attention = ProbAttention(mask_flag=False, \n",
    "                                                                                              factor=5, \n",
    "                                                                                              scale=None, \n",
    "                                                                                              attention_dropout=dropout,\n",
    "                                                                                              output_attention=output_attention),\n",
    "                                                                    input_dim = d_model,\n",
    "                                                                    output_dim = d_model_each_type, # number * (d_model/heads)\n",
    "                                                                    d_model = d_model_each_type,\n",
    "                                                                    n_heads = heads_each_type, # part heads\n",
    "                                                                    causal_kernel_size= causal_kernel_size,\n",
    "                                                                    value_kernel_size = value_kernel_size,\n",
    "                                                                    resid_pdrop=dropout)) # 这个思考一下？？？？？？？\n",
    "                else:\n",
    "                    self_attention_layer_list.append(AttentionLayer(attention = MaskAttention(mask_typ = type_attn, \n",
    "                                                                                              attention_dropout=dropout,\n",
    "                                                                                              output_attention=output_attention),\n",
    "                                                                    input_dim = d_model,\n",
    "                                                                    output_dim = d_model_each_type, # number * (d_model/heads)\n",
    "                                                                    d_model = d_model_each_type,\n",
    "                                                                    n_heads = heads_each_type, # part heads\n",
    "                                                                    causal_kernel_size= causal_kernel_size,\n",
    "                                                                    value_kernel_size = value_kernel_size,\n",
    "                                                                    resid_pdrop=dropout))\n",
    "\n",
    "            self.self_attention_layer_list = nn.ModuleList(self_attention_layer_list)\n",
    "            self.norm1 = nn.BatchNorm1d(d_model)\n",
    "        else:\n",
    "            self.self_attention_layer_list = None\n",
    "            self.norm1 = None         \n",
    "\n",
    "        # ----------- cross_attention_layer_types ---------------\n",
    "        cross_attention_layer_list = []\n",
    "        for type_attn in cross_attention_layer_types:\n",
    "            if type_attn==\"ProbMask\":\n",
    "                cross_attention_layer_list.append(AttentionLayer(attention = ProbAttention(mask_flag=False, \n",
    "                                                                                           factor=5, \n",
    "                                                                                           scale=None, \n",
    "                                                                                           attention_dropout=dropout,\n",
    "                                                                                           output_attention=output_attention),\n",
    "                                                                 input_dim = d_model,\n",
    "                                                                 output_dim = d_model_each_type, \n",
    "                                                                 d_model = d_model_each_type,\n",
    "                                                                 n_heads = heads_each_type, \n",
    "                                                                 causal_kernel_size= causal_kernel_size,\n",
    "                                                                 value_kernel_size = value_kernel_size,\n",
    "                                                                 resid_pdrop=dropout)) # 这个思考一下？？？？？？？\n",
    "            else:\n",
    "                cross_attention_layer_list.append(AttentionLayer(attention = MaskAttention(mask_typ = type_attn, \n",
    "                                                                                           attention_dropout=dropout,\n",
    "                                                                                           output_attention=output_attention),\n",
    "                                                                 input_dim = d_model,\n",
    "                                                                 output_dim = d_model_each_type,\n",
    "                                                                 d_model = d_model_each_type,\n",
    "                                                                 n_heads = heads_each_type, \n",
    "                                                                 causal_kernel_size= causal_kernel_size,\n",
    "                                                                 value_kernel_size = value_kernel_size,\n",
    "                                                                 resid_pdrop=dropout))\n",
    "\n",
    "\n",
    "        self.cross_attention_layer_list = nn.ModuleList(cross_attention_layer_list)\n",
    "        self.norm2 = nn.BatchNorm1d(d_model)\n",
    "        \n",
    "        \n",
    "        d_ff = d_ff or 2*d_model\n",
    "        self.forward_kernel_size = forward_kernel_size\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels =d_model, \n",
    "                               out_channels=d_ff, \n",
    "                               kernel_size=self.forward_kernel_size)\n",
    "        \n",
    "        self.activation1 = F.relu if activation == \"relu\" else F.gelu\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels = d_ff, \n",
    "                               out_channels= d_model, \n",
    "                               kernel_size=self.forward_kernel_size)\n",
    "        \n",
    "        self.activation2 = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "        self.norm3 = nn.BatchNorm1d(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, cross):\n",
    "        # self attention\n",
    "        if len(self.self_attention_layer_types)>0:\n",
    "            attns = []\n",
    "            outs = []\n",
    "            for attn in self.self_attention_layer_list:\n",
    "                out_value, out_attn = attn(x,x,x)\n",
    "                attns.append(out_attn)\n",
    "                outs.append(out_value) \n",
    "            new_x =torch.cat(outs,dim=-1) # gen ju C die jia\n",
    "            # attention 输出的新value 肯定是 B L C的形状\n",
    "            attn = torch.cat(attns,dim=1)\n",
    "            x = x + self.dropout(new_x) # B L C  \n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = self.norm1(x)\n",
    "            x = x.permute(0, 2, 1)\n",
    "            \n",
    "        # cross attention            \n",
    "        attns = []\n",
    "        outs = []\n",
    "        for attn in self.cross_attention_layer_list:\n",
    "            out_value, out_attn = attn(x,cross,cross)\n",
    "            attns.append(out_attn)\n",
    "            outs.append(out_value) \n",
    "        new_x =torch.cat(outs,dim=-1) # gen ju C die jia\n",
    "        # attention 输出的新value 肯定是 B L C的形状\n",
    "        attn = torch.cat(attns,dim=1)\n",
    "        x = x + self.dropout(new_x) # B L C  \n",
    "\n",
    "            \n",
    "        y = x = self.norm2(x.permute(0, 2, 1))\n",
    "\n",
    "        forward_padding_size = int(self.forward_kernel_size/2) \n",
    "        paddding_y  = nn.functional.pad(y, \n",
    "                                        pad=(forward_padding_size, forward_padding_size),\n",
    "                                        mode='replicate') \n",
    "        y = self.dropout(self.activation1(self.conv1(paddding_y)))    \n",
    "\n",
    "\n",
    "        paddding_y  = nn.functional.pad(y, \n",
    "                                        pad=(forward_padding_size, forward_padding_size),\n",
    "                                        mode='replicate')           \n",
    "        y = self.dropout(self.activation2(self.conv2(paddding_y)))\n",
    "\n",
    "        y = self.norm3(x+y).permute(0, 2, 1)  # B L  C \n",
    "\n",
    "        return y, attn\n",
    "    \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_layers = nn.ModuleList(decoder_layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, cross):\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            x, _ = layer(x, cross)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"LocalSymmetry\",\"LocLogSymmetry\",\"ProbMask\"]\n",
    "d_layers =3\n",
    "decoder_list = []\n",
    "for l in range(d_layers):\n",
    "    if l > 0:\n",
    "        a=[]\n",
    "    decoder_list.append(DecoderLayer(self_attention_layer_types = a,\n",
    "                                     cross_attention_layer_types= [\"LocalSymmetry\",\"LocLogSymmetry\",\"ProbMask\"],\n",
    "                                     d_model = 99,\n",
    "                                     n_heads = 3,\n",
    "                                     d_ff=None,\n",
    "                                     dropout=0.1,\n",
    "                                     activation='relu',\n",
    "                                     forward_kernel_size=1,\n",
    "                                     value_kernel_size=1,\n",
    "                                     causal_kernel_size=3,\n",
    "                                     output_attention=True))\n",
    "d = Decoder(decoder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0): DecoderLayer(\n",
       "      (self_attention_layer_list): ModuleList(\n",
       "        (0): AttentionLayer(\n",
       "          (query_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (key_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (value_projection): Conv1d(99, 33, kernel_size=(1,), stride=(1,))\n",
       "          (inner_attention): MaskAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_projection): Conv1d(33, 33, kernel_size=(1,), stride=(1,))\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): AttentionLayer(\n",
       "          (query_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (key_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (value_projection): Conv1d(99, 33, kernel_size=(1,), stride=(1,))\n",
       "          (inner_attention): MaskAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_projection): Conv1d(33, 33, kernel_size=(1,), stride=(1,))\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): AttentionLayer(\n",
       "          (query_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (key_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (value_projection): Conv1d(99, 33, kernel_size=(1,), stride=(1,))\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_projection): Conv1d(33, 33, kernel_size=(1,), stride=(1,))\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): BatchNorm1d(99, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cross_attention_layer_list): ModuleList(\n",
       "        (0): AttentionLayer(\n",
       "          (query_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (key_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (value_projection): Conv1d(99, 33, kernel_size=(1,), stride=(1,))\n",
       "          (inner_attention): MaskAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_projection): Conv1d(33, 33, kernel_size=(1,), stride=(1,))\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): AttentionLayer(\n",
       "          (query_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (key_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (value_projection): Conv1d(99, 33, kernel_size=(1,), stride=(1,))\n",
       "          (inner_attention): MaskAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_projection): Conv1d(33, 33, kernel_size=(1,), stride=(1,))\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): AttentionLayer(\n",
       "          (query_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (key_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (value_projection): Conv1d(99, 33, kernel_size=(1,), stride=(1,))\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_projection): Conv1d(33, 33, kernel_size=(1,), stride=(1,))\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): BatchNorm1d(99, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv1d(99, 198, kernel_size=(1,), stride=(1,))\n",
       "      (conv2): Conv1d(198, 99, kernel_size=(1,), stride=(1,))\n",
       "      (norm3): BatchNorm1d(99, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DecoderLayer(\n",
       "      (cross_attention_layer_list): ModuleList(\n",
       "        (0): AttentionLayer(\n",
       "          (query_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (key_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (value_projection): Conv1d(99, 33, kernel_size=(1,), stride=(1,))\n",
       "          (inner_attention): MaskAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_projection): Conv1d(33, 33, kernel_size=(1,), stride=(1,))\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): AttentionLayer(\n",
       "          (query_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (key_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (value_projection): Conv1d(99, 33, kernel_size=(1,), stride=(1,))\n",
       "          (inner_attention): MaskAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_projection): Conv1d(33, 33, kernel_size=(1,), stride=(1,))\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): AttentionLayer(\n",
       "          (query_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (key_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (value_projection): Conv1d(99, 33, kernel_size=(1,), stride=(1,))\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_projection): Conv1d(33, 33, kernel_size=(1,), stride=(1,))\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): BatchNorm1d(99, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv1d(99, 198, kernel_size=(1,), stride=(1,))\n",
       "      (conv2): Conv1d(198, 99, kernel_size=(1,), stride=(1,))\n",
       "      (norm3): BatchNorm1d(99, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): DecoderLayer(\n",
       "      (cross_attention_layer_list): ModuleList(\n",
       "        (0): AttentionLayer(\n",
       "          (query_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (key_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (value_projection): Conv1d(99, 33, kernel_size=(1,), stride=(1,))\n",
       "          (inner_attention): MaskAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_projection): Conv1d(33, 33, kernel_size=(1,), stride=(1,))\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): AttentionLayer(\n",
       "          (query_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (key_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (value_projection): Conv1d(99, 33, kernel_size=(1,), stride=(1,))\n",
       "          (inner_attention): MaskAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_projection): Conv1d(33, 33, kernel_size=(1,), stride=(1,))\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): AttentionLayer(\n",
       "          (query_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (key_projection): Conv1d(99, 33, kernel_size=(3,), stride=(1,))\n",
       "          (value_projection): Conv1d(99, 33, kernel_size=(1,), stride=(1,))\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_projection): Conv1d(33, 33, kernel_size=(1,), stride=(1,))\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): BatchNorm1d(99, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv1d(99, 198, kernel_size=(1,), stride=(1,))\n",
       "      (conv2): Conv1d(198, 99, kernel_size=(1,), stride=(1,))\n",
       "      (norm3): BatchNorm1d(99, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.encoder = Encoder([EncoderLayer(attention_layer_types = self.attention_layer_types,\n",
    "                                     d_model               = self.d_model,\n",
    "                                     n_heads               = self.n_heads,\n",
    "                                     d_ff                  = self.d_ff,\n",
    "                                     dropout               = self.dropout,\n",
    "                                     activation            = self.activation,\n",
    "                                     forward_kernel_size   = self.forward_kernel_size,\n",
    "                                     value_kernel_size     = self.value_kernel_size,\n",
    "                                     causal_kernel_size    = self.causal_kernel_size,\n",
    "                                     output_attention      = self.output_attention,\n",
    "                                     norm                  = self.norm,\n",
    "                                     se_block              = self.se_block) for l in range(self.e_layers)]\n",
    "                       ).double()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
