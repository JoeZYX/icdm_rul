{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joezh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\core\\__init__.py:29: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\joezh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "c:\\users\\joezh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "from models.Attention import AttentionLayer,MaskAttention,ProbAttention\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 self_attention_layer_types, \n",
    "                 cross_attention_layer_types, \n",
    "                 d_model,\n",
    "                 n_heads,                # 和上面一样，所以需要是 类型个数的倍数\n",
    "                 d_ff=None,              # 因为attention不改变维度，在最后forward的时候，中间过程的维度\n",
    "                 dropout=0.1,  \n",
    "                 activation=\"relu\", \n",
    "                 forward_kernel_size = 1, \n",
    "                 value_kernel_size=1,\n",
    "                 causal_kernel_size = 3,\n",
    "                 output_attention=True):\n",
    "                 #norm =\"layer\", \n",
    "                 #se_block =False):\n",
    "\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        nr_heads_type = len(cross_attention_layer_types)\n",
    "        heads_each_type = int(n_heads/nr_heads_type)\n",
    "        d_model_each_type = int(d_model/nr_heads_type)\n",
    "        \n",
    "        #self.norm = norm\n",
    "        #self.se_block = se_block\n",
    "        self.self_attention_layer_types = self_attention_layer_types\n",
    "        self.cross_attention_layer_types = cross_attention_layer_types\n",
    "        \n",
    "        \n",
    "        # 第一部分，进行attention\n",
    "        # ----------- self_attention_layer_types ---------------\n",
    "        if len(self.self_attention_layer_types)>0:\n",
    "            self_attention_layer_list = []\n",
    "            for type_attn in self_attention_layer_types:\n",
    "                if type_attn==\"ProbMask\":\n",
    "                    self_attention_layer_list.append(AttentionLayer(attention = ProbAttention(mask_flag=False, \n",
    "                                                                                              factor=5, \n",
    "                                                                                              scale=None, \n",
    "                                                                                              attention_dropout=dropout,\n",
    "                                                                                              output_attention=output_attention),\n",
    "                                                                    input_dim = d_model,\n",
    "                                                                    output_dim = d_model_each_type, \n",
    "                                                                    d_model = d_model_each_type,\n",
    "                                                                    n_heads = heads_each_type, \n",
    "                                                                    causal_kernel_size= causal_kernel_size,\n",
    "                                                                    value_kernel_size = value_kernel_size,\n",
    "                                                                    resid_pdrop=dropout)) # 这个思考一下？？？？？？？\n",
    "                else:\n",
    "                    self_attention_layer_list.append(AttentionLayer(attention = MaskAttention(mask_typ = type_attn, \n",
    "                                                                                              attention_dropout=dropout,\n",
    "                                                                                              output_attention=output_attention),\n",
    "                                                                    input_dim = d_model,\n",
    "                                                                    output_dim = d_model_each_type, \n",
    "                                                                    d_model = d_model_each_type,\n",
    "                                                                    n_heads = heads_each_type, \n",
    "                                                                    causal_kernel_size= causal_kernel_size,\n",
    "                                                                    value_kernel_size = value_kernel_size,\n",
    "                                                                    resid_pdrop=dropout))\n",
    "\n",
    "            self.self_attention_layer_list = nn.ModuleList(self_attention_layer_list)\n",
    "            self.norm1 = nn.BatchNorm1d(d_model)\n",
    "        else:\n",
    "            self.self_attention_layer_list = None\n",
    "            self.norm1 = None         \n",
    "\n",
    "        # ----------- cross_attention_layer_types ---------------\n",
    "        cross_attention_layer_list = []\n",
    "        for type_attn in cross_attention_layer_types:\n",
    "            if type_attn==\"ProbMask\":\n",
    "                cross_attention_layer_list.append(AttentionLayer(attention = ProbAttention(mask_flag=False, \n",
    "                                                                                           factor=5, \n",
    "                                                                                           scale=None, \n",
    "                                                                                           attention_dropout=dropout,\n",
    "                                                                                           output_attention=output_attention),\n",
    "                                                                 input_dim = d_model,\n",
    "                                                                 output_dim = d_model_each_type, \n",
    "                                                                 d_model = d_model_each_type,\n",
    "                                                                 n_heads = heads_each_type, \n",
    "                                                                 causal_kernel_size= causal_kernel_size,\n",
    "                                                                 value_kernel_size = value_kernel_size,\n",
    "                                                                 resid_pdrop=dropout)) # 这个思考一下？？？？？？？\n",
    "            else:\n",
    "                cross_attention_layer_list.append(AttentionLayer(attention = MaskAttention(mask_typ = type_attn, \n",
    "                                                                                           attention_dropout=dropout,\n",
    "                                                                                           output_attention=output_attention),\n",
    "                                                                 input_dim = d_model,\n",
    "                                                                 output_dim = d_model_each_type,\n",
    "                                                                 d_model = d_model_each_type,\n",
    "                                                                 n_heads = heads_each_type, \n",
    "                                                                 causal_kernel_size= causal_kernel_size,\n",
    "                                                                 value_kernel_size = value_kernel_size,\n",
    "                                                                 resid_pdrop=dropout))\n",
    "\n",
    "\n",
    "        self.cross_attention_layer_list = nn.ModuleList(cross_attention_layer_list)\n",
    "        self.norm2 = nn.BatchNorm1d(d_model)\n",
    "        \n",
    "        \n",
    "        d_ff = d_ff or 2*d_model\n",
    "        self.forward_kernel_size = forward_kernel_size\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels =d_model, \n",
    "                               out_channels=d_ff, \n",
    "                               kernel_size=self.forward_kernel_size)\n",
    "        \n",
    "        self.activation1 = F.relu if activation == \"relu\" else F.gelu\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels = d_ff, \n",
    "                               out_channels= d_model, \n",
    "                               kernel_size=self.forward_kernel_size)\n",
    "        \n",
    "        self.activation2 = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "        self.norm3 = nn.BatchNorm1d(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, cross):\n",
    "        # self attention\n",
    "        if len(self.self_attention_layer_types)>0:\n",
    "            attns = []\n",
    "            outs = []\n",
    "            for attn in self.self_attention_layer_list:\n",
    "                out_value, out_attn = attn(x,x,x)\n",
    "                attns.append(out_attn)\n",
    "                outs.append(out_value) \n",
    "            new_x =torch.cat(outs,dim=-1) # gen ju C die jia\n",
    "            # attention 输出的新value 肯定是 B L C的形状\n",
    "            attn = torch.cat(attns,dim=1)\n",
    "            x = x + self.dropout(new_x) # B L C  \n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = self.norm1(x)\n",
    "            x = x.permute(0, 2, 1)\n",
    "            \n",
    "        # cross attention            \n",
    "        attns = []\n",
    "        outs = []\n",
    "        for attn in self.cross_attention_layer_list:\n",
    "            out_value, out_attn = attn(x,cross,cross)\n",
    "            attns.append(out_attn)\n",
    "            outs.append(out_value) \n",
    "        new_x =torch.cat(outs,dim=-1) # gen ju C die jia\n",
    "        # attention 输出的新value 肯定是 B L C的形状\n",
    "        attn = torch.cat(attns,dim=1)\n",
    "        x = x + self.dropout(new_x) # B L C  \n",
    "\n",
    "            \n",
    "        y = x = self.norm2(x.permute(0, 2, 1))\n",
    "\n",
    "        forward_padding_size = int(self.forward_kernel_size/2) \n",
    "        paddding_y  = nn.functional.pad(y, \n",
    "                                        pad=(forward_padding_size, forward_padding_size),\n",
    "                                        mode='replicate') \n",
    "        y = self.dropout(self.activation1(self.conv1(paddding_y)))    \n",
    "\n",
    "\n",
    "        paddding_y  = nn.functional.pad(y, \n",
    "                                        pad=(forward_padding_size, forward_padding_size),\n",
    "                                        mode='replicate')           \n",
    "        y = self.dropout(self.activation2(self.conv2(paddding_y)))\n",
    "\n",
    "        y = self.norm3(x+y).permute(0, 2, 1)  # B L  C \n",
    "\n",
    "        return y, attn\n",
    "    \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_layers = nn.ModuleList(decoder_layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, cross):\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            x, _ = layer(x, cross)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"LocalSymmetry\",\"LocLogSymmetry\",\"ProbMask\"]\n",
    "d_layers =3\n",
    "decoder_list = []\n",
    "for l in range(d_layers):\n",
    "    if l > 0:\n",
    "        a=[]\n",
    "    decoder_list.append(DecoderLayer(self_attention_layer_types = a,\n",
    "                                     cross_attention_layer_types= [\"LocalSymmetry\",\"LocLogSymmetry\",\"ProbMask\"],\n",
    "                                     d_model = 99,\n",
    "                                     n_heads = 3,\n",
    "                                     d_ff=None,\n",
    "                                     dropout=0.1,\n",
    "                                     activation='relu',\n",
    "                                     forward_kernel_size=1,\n",
    "                                     value_kernel_size=1,\n",
    "                                     causal_kernel_size=3,\n",
    "                                     output_attention=True))\n",
    "d = Decoder(decoder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4692],\n",
      "         [-1.8913]],\n",
      "\n",
      "        [[-0.4577],\n",
      "         [ 0.4253]]], dtype=torch.float64)\n",
      "tensor([[[-0.2346],\n",
      "         [-0.9457]],\n",
      "\n",
      "        [[-0.2288],\n",
      "         [ 0.2127]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "input_ = torch.randn((2,2)).double()\n",
    "input_ = torch.unsqueeze(input_, 2)\n",
    "print(input_)\n",
    "input_ = torch.div(input_,2)\n",
    "print(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1520],\n",
      "         [ 0.0067],\n",
      "         [-0.2510],\n",
      "         [-1.2573],\n",
      "         [ 1.0762],\n",
      "         [-0.2074],\n",
      "         [ 0.0527],\n",
      "         [ 0.4877],\n",
      "         [ 0.4075],\n",
      "         [-0.2858]]], dtype=torch.float64)\n",
      "tensor([[[-0.6929, -0.7793,  0.9872],\n",
      "         [ 1.0117,  0.2832, -0.2550],\n",
      "         [-1.7767, -1.8039, -0.0401],\n",
      "         [-1.5432, -0.3968, -0.6816],\n",
      "         [-0.5984,  1.2180, -1.1834],\n",
      "         [ 0.7962,  1.1860,  0.7298],\n",
      "         [ 1.5739,  0.0409, -0.8559],\n",
      "         [-1.1098,  0.6208,  0.2381],\n",
      "         [ 0.5526,  1.0809, -1.8835],\n",
      "         [-0.6191, -0.9629,  0.3219]]], dtype=torch.float64)\n",
      "tensor([[[-1.1520, -0.6929, -0.7793,  0.9872],\n",
      "         [ 0.0067,  1.0117,  0.2832, -0.2550],\n",
      "         [-0.2510, -1.7767, -1.8039, -0.0401],\n",
      "         [-1.2573, -1.5432, -0.3968, -0.6816],\n",
      "         [ 1.0762, -0.5984,  1.2180, -1.1834],\n",
      "         [-0.2074,  0.7962,  1.1860,  0.7298],\n",
      "         [ 0.0527,  1.5739,  0.0409, -0.8559],\n",
      "         [ 0.4877, -1.1098,  0.6208,  0.2381],\n",
      "         [ 0.4075,  0.5526,  1.0809, -1.8835],\n",
      "         [-0.2858, -0.6191, -0.9629,  0.3219]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "input_a = torch.randn((1,10,1)).double()\n",
    "print(input_a)\n",
    "input_b = torch.randn((1,10,3)).double()\n",
    "print(input_b)\n",
    "dec_in  = torch.cat([input_a,input_b],dim=-1)\n",
    "print(dec_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.encoder = Encoder([EncoderLayer(attention_layer_types = self.attention_layer_types,\n",
    "                                     d_model               = self.d_model,\n",
    "                                     n_heads               = self.n_heads,\n",
    "                                     d_ff                  = self.d_ff,\n",
    "                                     dropout               = self.dropout,\n",
    "                                     activation            = self.activation,\n",
    "                                     forward_kernel_size   = self.forward_kernel_size,\n",
    "                                     value_kernel_size     = self.value_kernel_size,\n",
    "                                     causal_kernel_size    = self.causal_kernel_size,\n",
    "                                     output_attention      = self.output_attention,\n",
    "                                     norm                  = self.norm,\n",
    "                                     se_block              = self.se_block) for l in range(self.e_layers)]\n",
    "                       ).double()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
